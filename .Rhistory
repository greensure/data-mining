cat("The standard deviation of iris SepalWidth is: ", sd(SepalWidth), "\n")
cat("The standard deviation of iris PetalLength is: ", sd(PetalLength), "\n")
cat("The standard deviation of iris PetalWidth is: ", sd(PetalWidth), "\n")
ggplot(iris, aes(x=Petal.Length)) +
geom_histogram(color="black", fill="white", binwidth = .8)
ggplot(iris, aes(x=Petal.Width)) +
geom_histogram(color="black", fill="white", binwidth = .2)
par(mfrow=c(2,2))
# Use the ggplot2 library from CRAN to create a colored boxplot for each feature, with a box-whisker per flower species.
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Petal.Length, color = Species))
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Petal.Width, color = Species))
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Sepal.Length, color = Species))
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Sepal.Width, color = Species))
# Load the trees sample dataset into R and produce a 5-number summary of each feature
trees = data.frame(trees)
trees
str(trees)
# produce a 5-number summary of each feature
cat("A five number summary of Girth is", fivenum(trees$Girth), "\n")
cat("A five number summary of Height is", fivenum(trees$Height), "\n")
cat("A five number summary of Height is", fivenum(trees$Volume), "\n")
cat("Output a 5-number summary of each feature in detail： ", "\n")
cat("summary(trees$Girth)： ", "\n")
summary(trees$Girth)
cat("summary(trees$Height): ", "\n")
summary(trees$Height)
cat("summary(trees$Volume): ", "\n")
summary(trees$Volume)
# Create a histogram of each variable
# which variables appear to be normally distributed based on visual inspection?
ggplot(trees, aes(x = Girth)) + geom_histogram(color = "black", fill = "grey", binwidth = 1)
ggplot(trees, aes(x = Height)) + geom_histogram(color = "black", fill = "blue", binwidth = 1)
ggplot(trees, aes(x = Volume)) + geom_histogram(color = "black", fill = "green", binwidth = 3)
# Do any variables exhibit positive or negative skewness?
# Do the values agree with the visual inspection?
# install.packages("moments")
library("moments")
cat("\nSkewness of Girth:",skewness(trees$Girth))
cat("\nSkewness of Height:",skewness(trees$Height))
cat("\nSkewness of Height:",skewness(trees$Volume))
# Load the auto-mpg sample dataset from the UCI Machine Learning Repository (auto-mpg.data) into R using a dataframe
# (Hint: You will need to use read.csv with url, and set the appropriate values for header,as.is, and sep).
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
df <- read.csv(file = url,
header = FALSE,
sep = "",
as.is = T,
stringsAsFactors = F,
col.names =  c("mpg","cylinders","displacement","horsepower","weight","acceleration", "model year", "origin", "car name"))
str(df)
head(df)
# Load the auto-mpg sample dataset from the UCI Machine Learning Repository (auto-mpg.data) into R using a dataframe
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
df <- read.csv(file = url,
header = FALSE,
sep = "",
as.is = T,
stringsAsFactors = F,
col.names =  c("mpg","cylinders","displacement","horsepower","weight","acceleration", "model year", "origin", "car name"))
str(df)
head(df)
# Use the as.numeric casting function to obtain the column as a numeric vector,
df$horsepower <-  as.numeric(df$horsepower)
# df$horsepower
horsePowerBase <- df$horsepower
# Orignial Mean
cat("Orignial Mean is", mean(df$horsepower, na.rm = TRUE), "\n")
horse_NA_median <- df$horsepower
# Replace all NA values with the median
horse_NA_median[which(is.na(df$horsepower))] <- median(df$horsepower, na.rm =TRUE)
# Use the as.numeric casting function to obtain the column as a numeric vector,
df$horsepower <-  as.numeric(df$horsepower)
# df$horsepower
horsePowerBase <- df$horsepower
# Orignial Mean
cat("Orignial Mean is", mean(df$horsepower, na.rm = TRUE), "\n")
horse_NA_median <- df$horsepower
# Replace all NA values with the median
horse_NA_median[which(is.na(df$horsepower))] <- median(df$horsepower, na.rm =TRUE)
# Load the Boston sample dataset into R using a dataframe
Boston <- data.frame(Boston)
Boston
names(Boston)
?Boston
# Load the Boston sample dataset into R using a dataframe
Boston <- data.frame(Boston)
head(Boston)
names(Boston)
?Boston
# Use lm to fit a regression between medv and lstat - plot the resulting fit and show a plot of fitted values vs. residuals
# medv: median value of owner-occupied homes in \$1000s.
# lstat: lower status of the population (percent).
lm.fit <- lm(medv~lstat, data=Boston )
lm.fit
par(mfrow = c(2,2))
# par(mfrow = c(1,1))
plot(lm.fit)
summary(lm.fit)
# Use the predict function to calculate values response values for lstat of 5, 10, and 15 - obtain confidence intervals as well as prediction intervals for the results
# lstat: lower status of the population (percent).
cat("Calculate values response values for lstat of 5, 10, and 15 - obtain confidence intervals", "\n")
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval ="confidence")
cat("Calculate values response values for lstat of 5, 10, and 15 - obtain prediction intervals", "\n")
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval ="prediction")
# Use the predict function to calculate values response values for lstat of 5, 10, and 15 - obtain confidence intervals as well as prediction intervals for the results
# lstat: lower status of the population (percent).
cat("Calculate values response values for lstat of 5, 10, and 15 - obtain confidence intervals", "\n")
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval ="confidence")
cat("Calculate values response values for lstat of 5, 10, and 15 - obtain prediction intervals", "\n")
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval ="prediction")
# Use the predict function to calculate values response values for lstat of 5, 10, and 15 - obtain confidence intervals as well as prediction intervals for the results
# lstat: lower status of the population (percent).
cat("Calculate values response values for lstat of 5, 10, and 15 - obtain confidence intervals", "\n")
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval ="confidence")
cat("Calculate values response values for lstat of 5, 10, and 15 - obtain prediction intervals", "\n")
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval ="prediction")
# Modify the regression to include lstat2 (as well lstat itself) and compare the R2 between the linear and non-linear fit - use ggplot2 and stat smooth to plot the relationship.
intercept <- coef(lm.fit)[1]
slope <- coef(lm.fit)[2]
fit2 <- lm(medv ~ lstat + I(lstat ^2), data=Boston)
summary(fit2)
# Modify the regression to include lstat2 (as well lstat itself) and compare the R2 between the linear and non-linear fit - use ggplot2 and stat smooth to plot the relationship.
intercept <- coef(lm.fit)[1]
slope <- coef(lm.fit)[2]
fit2 <- lm(medv ~ lstat + I(lstat ^2), data=Boston)
summary(fit2)
predictions <- data.frame(pred = predict(fit2, data.frame(lstat=1:30)))
ggplot(Boston, aes(x = lstat, y = medv)) + geom_point(alpha = 0.5) +
geom_line(data = predictions, aes(x = 1:dim(predictions)[1], y = pred), color = "red") +
geom_abline(intercept = intercept, slope = slope, color = "blue")
anova(fit2, lm.fit)
# Clean environment
rm(list=ls())
# Set the working Directory
setwd("D:/WorkSpace/RWorkSpace/HWDemo")
# Load libraries
library(tidyverse)
library(datasets)
library('ggplot2')
library (MASS)
library (ISLR)
# Load the iris sample dataset into R
iris = data.frame(iris)
head(iris)
str(iris)
# Create a boxplot of each of the 4 features
# boxplot(iris, xlab = "Attributes", ylab = "Values", main = "Iris")
ggplot(data=stack(iris), mapping = aes(x=ind, y = values)) + geom_boxplot()
# IQR of each of the 4 features
SepalLength <- iris$Sepal.Length
SepalWidth <- iris$Sepal.Width
PetalLength <- iris$Petal.Length
PetalWidth <- iris$Petal.Width
cat("\nIQR of SepalLength: ", IQR(SepalLength))
cat("\nIQR of SepalWidth: ", IQR(SepalWidth))
cat("\nIQR of PetalLength: ", IQR(PetalLength))
cat("\nIQR of PetalWidth: ", IQR(PetalWidth))
cat("\nThe feature with the largest empirical IQR is PetalLength = 3.5")
# Highlight the feature with the largest empirical IQR
cat("\nHighlight the feature with the largest empirical IQR.")
boxplot(iris$Petal.Length, main="The Largest IQR = 3.5", xlab="Petal Length", ylab ="Value", col="green" )
par(mfrow=c(2,2))
# Calculate the parametric standard deviation
cat("The standard deviation of iris SepalLength is: ", sd(SepalLength), "\n")
cat("The standard deviation of iris SepalWidth is: ", sd(SepalWidth), "\n")
cat("The standard deviation of iris PetalLength is: ", sd(PetalLength), "\n")
cat("The standard deviation of iris PetalWidth is: ", sd(PetalWidth), "\n")
ggplot(iris, aes(x=Petal.Length)) +
geom_histogram(color="black", fill="white", binwidth = .8)
ggplot(iris, aes(x=Petal.Width)) +
geom_histogram(color="black", fill="white", binwidth = .2)
par(mfrow=c(2,2))
# Use the ggplot2 library from CRAN to create a colored boxplot for each feature, with a box-whisker per flower species.
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Petal.Length, color = Species))
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Petal.Width, color = Species))
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Sepal.Length, color = Species))
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Sepal.Width, color = Species))
ggplot(data=iris) + geom_boxplot(mapping = aes(x=Petal.Length, y=Petal.Width, color = Species))
# Load the trees sample dataset into R and produce a 5-number summary of each feature
trees = data.frame(trees)
trees
str(trees)
# produce a 5-number summary of each feature
cat("A five number summary of Girth is", fivenum(trees$Girth), "\n")
cat("A five number summary of Height is", fivenum(trees$Height), "\n")
cat("A five number summary of Height is", fivenum(trees$Volume), "\n")
cat("Output a 5-number summary of each feature in detail： ", "\n")
cat("summary(trees$Girth)： ", "\n")
summary(trees$Girth)
cat("summary(trees$Height): ", "\n")
summary(trees$Height)
cat("summary(trees$Volume): ", "\n")
summary(trees$Volume)
# Create a histogram of each variable
# which variables appear to be normally distributed based on visual inspection?
ggplot(trees, aes(x = Girth)) + geom_histogram(color = "black", fill = "grey", binwidth = 1)
ggplot(trees, aes(x = Height)) + geom_histogram(color = "black", fill = "blue", binwidth = 1)
ggplot(trees, aes(x = Volume)) + geom_histogram(color = "black", fill = "green", binwidth = 3)
# Do any variables exhibit positive or negative skewness?
# Do the values agree with the visual inspection?
# install.packages("moments")
library("moments")
cat("\nSkewness of Girth:",skewness(trees$Girth))
cat("\nSkewness of Height:",skewness(trees$Height))
cat("\nSkewness of Height:",skewness(trees$Volume))
# Load the auto-mpg sample dataset from the UCI Machine Learning Repository (auto-mpg.data) into R using a dataframe
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
df <- read.csv(file = url,
header = FALSE,
sep = "",
as.is = T,
stringsAsFactors = F,
col.names =  c("mpg","cylinders","displacement","horsepower","weight","acceleration", "model year", "origin", "car name"))
str(df)
head(df)
# Use the as.numeric casting function to obtain the column as a numeric vector,
df$horsepower <-  as.numeric(df$horsepower)
# df$horsepower
horsePowerBase <- df$horsepower
# Orignial Mean
cat("Orignial Mean is", mean(df$horsepower, na.rm = TRUE), "\n")
horse_NA_median <- df$horsepower
# Replace all NA values with the median
horse_NA_median[which(is.na(df$horsepower))] <- median(df$horsepower, na.rm =TRUE)
# Mean when NA = median
cat("Mean when NA = median is", mean(horse_NA_median))
# How does this affec the value obtained for the mean vs the original mean when the records were ignored?
# Load the Boston sample dataset into R using a dataframe
Boston <- data.frame(Boston)
head(Boston)
names(Boston)
?Boston
# Use lm to fit a regression between medv and lstat - plot the resulting fit and show a plot of fitted values vs. residuals
# medv: median value of owner-occupied homes in \$1000s.
# lstat: lower status of the population (percent).
lm.fit <- lm(medv~lstat, data=Boston )
lm.fit
par(mfrow = c(2,2))
# par(mfrow = c(1,1))
plot(lm.fit)
summary(lm.fit)
# Use the predict function to calculate values response values for lstat of 5, 10, and 15 - obtain confidence intervals as well as prediction intervals for the results
# lstat: lower status of the population (percent).
cat("Calculate values response values for lstat of 5, 10, and 15 - obtain confidence intervals", "\n")
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval ="confidence")
cat("Calculate values response values for lstat of 5, 10, and 15 - obtain prediction intervals", "\n")
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval ="prediction")
# Modify the regression to include lstat2 (as well lstat itself) and compare the R2 between the linear and non-linear fit - use ggplot2 and stat smooth to plot the relationship.
intercept <- coef(lm.fit)[1]
slope <- coef(lm.fit)[2]
fit2 <- lm(medv ~ lstat + I(lstat ^2), data=Boston)
summary(fit2)
predictions <- data.frame(pred = predict(fit2, data.frame(lstat=1:30)))
ggplot(Boston, aes(x = lstat, y = medv)) + geom_point(alpha = 0.5) +
geom_line(data = predictions, aes(x = 1:dim(predictions)[1], y = pred), color = "red") +
geom_abline(intercept = intercept, slope = slope, color = "blue")
anova(fit2, lm.fit)
# Clean environment
rm(list=ls())
# Set the working Directory
setwd("D:/WorkSpace/RWorkSpace/HWDemo")
# Load libraries
library(tidyverse)
library(datasets)
library('ggplot2')
library (MASS)
library (ISLR)
# Load the iris sample dataset into R
iris = data.frame(iris)
head(iris)
str(iris)
# Clean environment
rm(list=ls())
# Set the working Directory
setwd("D:/WorkSpace/RWorkSpace/HWDemo")
# Load libraries
library(caret)
library(corrplot)
library(pROC)
library(e1071)
# library(dplyr)
library(ROCR)
# Clean environment
rm(list=ls())
# Set the working Directory
setwd("D:/WorkSpace/RWorkSpace/HWDemo")
# Load libraries
library(tidyverse)
library(datasets)
library('ggplot2')
library (MASS)
library (ISLR)
# Load the iris sample dataset into R
iris = data.frame(iris)
head(iris)
str(iris)
# Create a boxplot of each of the 4 features
# boxplot(iris, xlab = "Attributes", ylab = "Values", main = "Iris")
ggplot(data=stack(iris), mapping = aes(x=ind, y = values)) + geom_boxplot()
# IQR of each of the 4 features
SepalLength <- iris$Sepal.Length
SepalWidth <- iris$Sepal.Width
PetalLength <- iris$Petal.Length
PetalWidth <- iris$Petal.Width
cat("\nIQR of SepalLength: ", IQR(SepalLength))
cat("\nIQR of SepalWidth: ", IQR(SepalWidth))
cat("\nIQR of PetalLength: ", IQR(PetalLength))
cat("\nIQR of PetalWidth: ", IQR(PetalWidth))
cat("\nThe feature with the largest empirical IQR is PetalLength = 3.5")
# Highlight the feature with the largest empirical IQR
cat("\nHighlight the feature with the largest empirical IQR.")
boxplot(iris$Petal.Length, main="The Largest IQR = 3.5", xlab="Petal Length", ylab ="Value", col="green" )
par(mfrow=c(2,2))
# Calculate the parametric standard deviation
cat("The standard deviation of iris SepalLength is: ", sd(SepalLength), "\n")
cat("The standard deviation of iris SepalWidth is: ", sd(SepalWidth), "\n")
cat("The standard deviation of iris PetalLength is: ", sd(PetalLength), "\n")
cat("The standard deviation of iris PetalWidth is: ", sd(PetalWidth), "\n")
ggplot(iris, aes(x=Petal.Length)) +
geom_histogram(color="black", fill="white", binwidth = .8)
ggplot(iris, aes(x=Petal.Width)) +
geom_histogram(color="black", fill="white", binwidth = .2)
par(mfrow=c(2,2))
# Use the ggplot2 library from CRAN to create a colored boxplot for each feature, with a box-whisker per flower species.
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Petal.Length, color = Species))
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Petal.Width, color = Species))
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Sepal.Length, color = Species))
ggplot(data = iris) + geom_boxplot(mapping = aes(x = Species, y = Sepal.Width, color = Species))
ggplot(data=iris) + geom_boxplot(mapping = aes(x=Petal.Length, y=Petal.Width, color = Species))
# Load the trees sample dataset into R and produce a 5-number summary of each feature
trees = data.frame(trees)
trees
str(trees)
# produce a 5-number summary of each feature
cat("A five number summary of Girth is", fivenum(trees$Girth), "\n")
cat("A five number summary of Height is", fivenum(trees$Height), "\n")
cat("A five number summary of Height is", fivenum(trees$Volume), "\n")
cat("Output a 5-number summary of each feature in detail： ", "\n")
cat("summary(trees$Girth)： ", "\n")
summary(trees$Girth)
cat("summary(trees$Height): ", "\n")
summary(trees$Height)
cat("summary(trees$Volume): ", "\n")
summary(trees$Volume)
# Create a histogram of each variable
# which variables appear to be normally distributed based on visual inspection?
ggplot(trees, aes(x = Girth)) + geom_histogram(color = "black", fill = "grey", binwidth = 1)
ggplot(trees, aes(x = Height)) + geom_histogram(color = "black", fill = "blue", binwidth = 1)
ggplot(trees, aes(x = Volume)) + geom_histogram(color = "black", fill = "green", binwidth = 3)
# Do any variables exhibit positive or negative skewness?
# Do the values agree with the visual inspection?
# install.packages("moments")
library("moments")
cat("\nSkewness of Girth:",skewness(trees$Girth))
cat("\nSkewness of Height:",skewness(trees$Height))
cat("\nSkewness of Height:",skewness(trees$Volume))
# Load the auto-mpg sample dataset from the UCI Machine Learning Repository (auto-mpg.data) into R using a dataframe
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
df <- read.csv(file = url,
header = FALSE,
sep = "",
as.is = T,
stringsAsFactors = F,
col.names =  c("mpg","cylinders","displacement","horsepower","weight","acceleration", "model year", "origin", "car name"))
str(df)
head(df)
# Use the as.numeric casting function to obtain the column as a numeric vector,
df$horsepower <-  as.numeric(df$horsepower)
# df$horsepower
horsePowerBase <- df$horsepower
# Orignial Mean
cat("Orignial Mean is", mean(df$horsepower, na.rm = TRUE), "\n")
horse_NA_median <- df$horsepower
# Replace all NA values with the median
horse_NA_median[which(is.na(df$horsepower))] <- median(df$horsepower, na.rm =TRUE)
# Mean when NA = median
cat("Mean when NA = median is", mean(horse_NA_median))
# How does this affec the value obtained for the mean vs the original mean when the records were ignored?
# Load the Boston sample dataset into R using a dataframe
Boston <- data.frame(Boston)
head(Boston)
names(Boston)
?Boston
# Use lm to fit a regression between medv and lstat - plot the resulting fit and show a plot of fitted values vs. residuals
# medv: median value of owner-occupied homes in \$1000s.
# lstat: lower status of the population (percent).
lm.fit <- lm(medv~lstat, data=Boston )
lm.fit
par(mfrow = c(2,2))
# par(mfrow = c(1,1))
plot(lm.fit)
summary(lm.fit)
# Use the predict function to calculate values response values for lstat of 5, 10, and 15 - obtain confidence intervals as well as prediction intervals for the results
# lstat: lower status of the population (percent).
cat("Calculate values response values for lstat of 5, 10, and 15 - obtain confidence intervals", "\n")
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval ="confidence")
cat("Calculate values response values for lstat of 5, 10, and 15 - obtain prediction intervals", "\n")
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval ="prediction")
# Modify the regression to include lstat2 (as well lstat itself) and compare the R2 between the linear and non-linear fit - use ggplot2 and stat smooth to plot the relationship.
intercept <- coef(lm.fit)[1]
slope <- coef(lm.fit)[2]
fit2 <- lm(medv ~ lstat + I(lstat ^2), data=Boston)
summary(fit2)
predictions <- data.frame(pred = predict(fit2, data.frame(lstat=1:30)))
ggplot(Boston, aes(x = lstat, y = medv)) + geom_point(alpha = 0.5) +
geom_line(data = predictions, aes(x = 1:dim(predictions)[1], y = pred), color = "red") +
geom_abline(intercept = intercept, slope = slope, color = "blue")
anova(fit2, lm.fit)
# Clean environment
rm(list=ls())
# Set the working Directory
setwd("D:/WorkSpace/RWorkSpace/HWDemo")
# Load libraries
library(caret)
library(corrplot)
library(pROC)
library(e1071)
# library(dplyr)
library(ROCR)
# Clean environment
rm(list=ls())
# Set the working Directory
setwd("D:/WorkSpace/RWorkSpace/HWDemo")
# Load libraries
library(caret)
library(corrplot)
library(pROC)
library(e1071)
# library(dplyr)
library(ROCR)
# Load the abalone sample dataset from the UCI Machine Learning Repository
abaloneUrl <- "https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data"
abalone <- read.csv(abaloneUrl,
header = F,
sep = ",",
stringsAsFactors = TRUE,
# Is there an easy way to read names by programming? Not manual input
col.names = c('Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings'))
str(abalone)
head(abalone)
# Check classes
xtabs(~Sex, data = abalone)
# 1. Remove all observations in the Infant category, keeping the Male/Female classes.
df.NoInfant <- subset(abalone, Sex != 'I')
df.NoInfant$Sex <- factor(df.NoInfant$Sex)
# 2. Data Partition: Using the caret package, use createDataPartition to perform an 80/20 test-train split
set.seed(1)
index <- createDataPartition(df.NoInfant$Sex, p = 0.2, list = FALSE)
testAbalone <- df.NoInfant[index,]
trainAbalone <- df.NoInfant[-index,]
# Logistic regression R model
# Fit a logistic regression using all feature variables via glm, and observe which predictors are relevant
# glm indicates generalized linear model and family is binomial because Sex has only F, M
modelLogisticReg <- glm(Sex ~ ., data = trainAbalone, family = binomial)
summary(modelLogisticReg)
# For P value, more stars indicate more statistical significance.
# According to the output, Shucked.weight, Diameter is statistically significant
confint(modelLogisticReg)
predTemp <- predict(modelLogisticReg, testAbalone, type = "response")
head(predTemp)
pred <- ifelse(predTemp >= 0.5, 'M', 'F')
# pred <- ifelse(predTemp >= 0.5, 'M', 'F', type="response")
# pred
tab1 <- table(Predicted = pred, Actual = testAbalone$Sex)
tab1
# Based on 96 + 215 = 311 correct classifications and 91 + 166 = 257 misclassifications.
# the misclassifications error rate
misclassifications <- 1 - sum(diag(tab1))/sum(tab1)
misclassifications
predTemp <- predict(modelLogisticReg, testAbalone, type = "response")
head(predTemp)
# Based on 96 + 215 = 311 correct classifications and 91 + 166 = 257 misclassifications.
# the misclassifications error rate
misclassifications <- 1 - sum(diag(tab1))/sum(tab1)
misclassifications
confusionMatrix(as.factor(pred), testAbalone$Sex)
plot(roc(testAbalone$Sex, predTemp))
# AUC
auc <- auc(testAbalone$Sex, predTemp)
cat("AUC =", auc, "\n")
# prediction
f.pred <- prediction(predTemp, testAbalone$Sex)
# performance
f.perf <- performance(f.pred, "tpr", "fpr")
plot(f.perf, colorize=T, lwd=3)
abline(0,1)
temp <- corrplot.mixed(cor(df.NoInfant[,2:8]))
temp
# Load the mushroom sample dataset from the UCI Machine Learning Repository (agaricus-lepiota.data) into R using a dataframe
urlMushroom = "https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data"
mushroom <- read.csv(urlMushroom,
header = FALSE,
sep = ",",
stringsAsFactors = TRUE)
head(mushroom)
# There are missing values with a ? character, you will have to explain your handling of these
# No.12 stalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s
# Step 1 Get the index of missing value
valuesMissing <- which(mushroom$V12 == '?')
mushroom_miss <- mushroom[-c(valuesMissing),]
# Step 2 Find mode via table() function
# According to the output, b is the mode.
mode <- (table(as.vector(mushroom_miss$V12)))
# Step 3 Replace ? via mode which is 'b'
mushroom_replace <- mushroom
mushroom_replace$V12[valuesMissing] <- 'b'
indexMiss <- sample(1:nrow(mushroom_miss),size=0.8*nrow(mushroom_miss))
trainMiss <- mushroom[indexMiss,]
testMiss <- mushroom[-indexMiss,]
indexReplace <- sample(1:nrow(mushroom_replace),size=0.8*nrow(mushroom_replace))
trainReplace <- mushroom[indexReplace,]
testReplace <- mushroom[-indexReplace,]
